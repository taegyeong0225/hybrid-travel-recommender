# -*- coding: utf-8 -*-
"""G_code_ver1228

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s_9uB4_A9RyPjYUw0294Gb2tSmVGWWnV

#1. 전처리

## 라이브러리 및 raw 데이터 불러오기

라이브러리 호출
"""

# 패키지 설치
# !pip install scikit-optimize
# !pip install catboost
# !pip install imblearn
# !pip install sklearn
# !pip install skopt
# !pip install preprocessing

"""raw data 호출"""

from datetime import datetime

# Get the current timestamp
timestamp = datetime.now()

# Print the timestamp
print("Current Timestamp:", timestamp)

# 필수 라이브러리 불러오기
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score

import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# 현재 위치는 /app/app/ml 
DATA_DIR = os.path.join(BASE_DIR, "1.inputdata")
PREPROC_DIR = os.path.join(BASE_DIR, "2.preprocessed")
SAVEMODEL_DIR = os.path.join(BASE_DIR, "4.SaveModel")

visit_G = pd.read_csv(os.path.join(DATA_DIR, "tn_visit_area_info_G.csv"))

id_list=['G']

visit_data_list=[visit_G]

for id,visit_area_info in zip(id_list,visit_data_list):
  # 관광지 선택
  visit_info = visit_area_info[ (visit_area_info['VISIT_AREA_TYPE_CD'] == 1) |
   (visit_area_info['VISIT_AREA_TYPE_CD'] == 2) |(visit_area_info['VISIT_AREA_TYPE_CD'] == 3) | (visit_area_info['VISIT_AREA_TYPE_CD'] == 4) |
    (visit_area_info['VISIT_AREA_TYPE_CD'] == 5) | (visit_area_info['VISIT_AREA_TYPE_CD'] == 6) |(visit_area_info['VISIT_AREA_TYPE_CD'] == 7) |
     (visit_area_info['VISIT_AREA_TYPE_CD'] == 8)]
  visit_info = visit_info.groupby('VISIT_AREA_NM').filter(lambda x: len(x) > 1)
  visit_info=visit_info.reset_index(drop = True)

visit_final_G=visit_info

visit_final_G['ratings'] = visit_final_G[['DGSTFN', 'REVISIT_INTENTION', 'RCMDTN_INTENTION']].mean(axis=1)

visit_final_G['TRAVELER_ID'] = visit_final_G['TRAVEL_ID'].str.split('_').str[1]

"""###세부 전처리

####C권역
"""

visit_final_G['SIDO'] = visit_final_G['LOTNO_ADDR'].str.split().str[0]

dfg=visit_final_G
# Group by 'FIRST_WORD' and find the most frequent 'VISIT_AREA_NM' for each group
most_frequent_visits = dfg.groupby('LOTNO_ADDR')['VISIT_AREA_NM'].agg(lambda x: x.mode().iloc[0]).reset_index()

# Merge the most frequent values back to the original DataFrame based on 'FIRST_WORD'
dfg = dfg.merge(most_frequent_visits, on='LOTNO_ADDR', how='left', suffixes=('', '_most_frequent'))

# Update 'VISIT_AREA_NM' with the most frequent values
dfg['VISIT_AREA_NM'] = dfg['VISIT_AREA_NM_most_frequent'].fillna(dfg['VISIT_AREA_NM'])

# Drop temporary columns used for grouping and merging
dfg.drop(columns=['VISIT_AREA_NM_most_frequent'], inplace=True)

dfg[['TRAVELER_ID', 'VISIT_AREA_NM','ratings','SIDO']]

"""####후처리"""

df1 = dfg.rename(columns={'TRAVELER_ID': 'userID','VISIT_AREA_NM': 'itemID','ratings': 'rating'})

df1=df1[['userID','itemID','rating','SIDO']]

df1.to_csv(os.path.join(PREPROC_DIR, "dfG.csv"))

"""#모델 수정"""

# !pip install surprise

from datetime import datetime

# Get the current timestamp
timestamp = datetime.now()

# Print the timestamp
print("Current Timestamp:", timestamp)

import pandas as pd
import numpy as np
import joblib
import surprise
import warnings
# from tqdm import tqdm

from surprise import SVD
from surprise.model_selection import cross_validate, GridSearchCV
from surprise import Dataset
from surprise.dataset import DatasetAutoFolds
from surprise import accuracy
from surprise.model_selection import train_test_split
from surprise import Reader

reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df1[['userID', 'itemID', 'rating']], reader)

algo = SVD()
param_grid = {'n_factors':[50,100,200],'n_epochs': [10,50], 'lr_all': [0.01,0.1], 'reg_all':[0.01,0.1], 'reg_bu':[0.01,0.1],'reg_bi':[0.01,0.1]}
grid = GridSearchCV(SVD, param_grid, measures=['RMSE', 'MAE'], cv=5, n_jobs=-1, joblib_verbose= 10)
grid.fit(data=data)

print(grid.best_score['rmse'])
print(grid.best_params['rmse'])

from surprise.model_selection import train_test_split

# 예상 점수 범위 설정
reader = Reader(line_format='user item rating timestamp', sep='\t')
reader = Reader(rating_scale= (1,5))

# 데이터 불러오기
data = Dataset.load_from_df(df1[['userID', 'itemID', 'rating']], reader)

# 훈련데이터, 테스트 데이터로 나누기
trainset, testset = train_test_split(data, test_size= 0.2, random_state = 42)

testset_df = pd.DataFrame(testset, columns=['userID', 'itemID', 'rating'])

import pickle
algo = SVD(n_factors=50, lr_all=0.01, reg_all=0.1, n_epochs=50, reg_bu=0.1, reg_bi=0.1)
algo.fit(trainset)
cross_validate(algo=algo, data=data, measures=['RMSE', 'MAE'], cv=10, verbose=True, n_jobs=-1)

# Specify the file path including the directory where you want to save the model
file_path = os.path.join(SAVEMODEL_DIR, 'model/svd_model_G.pkl')
# Save the model to the specified .pkl file
with open(file_path, 'wb') as file:
    pickle.dump(algo, file)

loaded_model1 = joblib.load(os.path.join(SAVEMODEL_DIR, 'model/svd_model_G.pkl'))

predictions=loaded_model1.test(testset)

prediction_data = []

for uid, iid, true_r, est, _ in predictions:
    prediction_data.append({'userID': uid, 'itemID': iid, 'true_rating': true_r, 'predicted_rating': est})


print(prediction_data)


# Convert the list of dictionaries into a DataFrame
predictions_df_G = pd.DataFrame(prediction_data)

itemID_to_SIDO = df1.set_index('itemID')['SIDO'].to_dict()
predictions_df_G['SIDO'] = predictions_df_G['itemID'].map(itemID_to_SIDO)

predictions_df_G['true_rec']=np.nan
predictions_df_G['est_rec']=np.nan

mean_true_rating = predictions_df_G['true_rating'].mean()
mean_predicted_rating = predictions_df_G['predicted_rating'].mean()
# Set 'true_rec' column based on the condition
predictions_df_G['true_rec'] = np.where(predictions_df_G['true_rating'] > mean_true_rating, 1, 0)

# Set 'est_rec' column based on the condition
predictions_df_G['est_rec'] = np.where(predictions_df_G['predicted_rating'] > mean_predicted_rating, 1, 0)

grouped = predictions_df_G.groupby(['userID', 'true_rec'])

# Iterate through groups
for (user_id, true_rec), group in grouped:
    # Check if 'true_rec' is 1
    if true_rec == 1:
        # Count the occurrences of each 'SIDO' value
        sido_counts = group['SIDO'].value_counts()

        # Get the majority 'SIDO' value
        majority_sido = sido_counts.index[0] if len(sido_counts) > 0 else None

        # Set 'true_rec' to 1 for the majority 'SIDO' and 0 for others
        predictions_df_G.loc[group.index, 'true_rec'] = (group['SIDO'] == majority_sido).astype(int)

grouped = predictions_df_G.groupby(['userID', 'predicted_rating'])

# Iterate through groups
for (user_id, pred_rec), group in grouped:
    if pred_rec == 1:
        # Count the occurrences of each 'SIDO' value
        sido_counts = group['SIDO'].value_counts()

        # Get the majority 'SIDO' value
        majority_sido = sido_counts.index[0] if len(sido_counts) > 0 else None

        # Set 'true_rec' to 1 for the majority 'SIDO' and 0 for others
        predictions_df_G.loc[group.index, 'pred_rec'] = (group['SIDO'] == majority_sido).astype(int)

print(predictions_df_G)


def recall5_calculator(df):
  df_sorted = df.sort_values(by=['userID', 'true_rec', 'predicted_rating'], ascending=[True, False, False])
  # Function to calculate Recall@K for a given user
  def calculate_recall_at_k(user_data, k=5):
      # Count the number of true positives (interest) and total relevant items
      true_positives = user_data['true_rec'].sum()
      total_interest_items = user_data['true_rec'].sum()

      # Count the number of recommended items
      recommended_items = user_data['est_rec'].head(k).sum()

      # Calculate Recall@K
      recall_at_k = true_positives / total_interest_items if total_interest_items > 0 else 0

      return recall_at_k
  recall_at_5_values = df_sorted.groupby('userID').apply(lambda x: calculate_recall_at_k(x, k=5))

  # Calculate average Recall@5 across all users
  average_recall_at_5 = recall_at_5_values.mean()

  return average_recall_at_5

# Print average Recall@5
a=recall5_calculator(predictions_df_G)
print('수도권 권역의 recall@5:'+str(a))

filtered_predictions_df_G = predictions_df_G[predictions_df_G['est_rec'] == 1]

# Group by 'userID' and select top 5 items based on 'predicted_rating'
top_5_items_per_user = (
    filtered_predictions_df_G.groupby('userID')
    .apply(lambda x: x.nlargest(5, 'predicted_rating'))
    .reset_index(drop=True)
)

# Create the recommendation column as a list of itemIDs for each userID
recommendation_lists = top_5_items_per_user.groupby('userID')['itemID'].apply(list).reset_index()

# Rename the columns as userID and recommendation
recommendation_lists.columns = ['userID', 'recommendation']

# Ensure that each list contains at most 5 itemIDs
recommendation_lists['recommendation'] = recommendation_lists['recommendation'].apply(lambda x: x[:5])

# Create the final DataFrame df_E_final
df_G_final = recommendation_lists.copy()
df_G_final.to_csv(os.path.join(SAVEMODEL_DIR, 'result/testset_output/G_test_ouput.csv'))

dataset = df1.drop('SIDO',axis=1).values.tolist()
sido_df=df1[['itemID', 'SIDO']].drop_duplicates()
predictions = loaded_model1.test(dataset)

# 예측 결과를 데이터프레임으로 변환
result_dfc = pd.DataFrame(predictions, columns=['user_id', 'item_id', 'actual_rating', 'predicted_rating', 'details'])

# 필요한 컬럼만 선택 (user_id, item_id, actual_rating, predicted_rating)
result_dfc = result_dfc[['user_id', 'item_id', 'actual_rating', 'predicted_rating']]

dfc_pivot=result_dfc.pivot_table('predicted_rating', index='user_id', columns='item_id').fillna(0)

user_item_matrix = df1.pivot_table(index = 'userID', columns = 'itemID', values = 'rating').fillna(0)

# 이미 방문한 숙박 업소 제외하고 추천
recommendations = []
for idx, user in enumerate(user_item_matrix.index):
    # 해당 사용자가 방문한 숙박 업소
    applied_accs = set(user_item_matrix.loc[user][user_item_matrix.loc[user] != 0].index)

    sorted_acc_indices = dfc_pivot.iloc[idx].argsort()[::-1]
    recommended_accs = [acc for acc in user_item_matrix.columns[sorted_acc_indices] if acc not in applied_accs][:5]
    for acc in recommended_accs:
        recommendations.append([user, acc])

top_recommendations = pd.DataFrame(recommendations, columns=['userID', 'itemID'])
top_recommendations_with_sido = pd.merge(top_recommendations, sido_df, on='itemID', how='left')

def filter_majority_sido(group):
    sido_counts = group['SIDO'].value_counts()
    if len(sido_counts) > 0:
        majority_sido = sido_counts.idxmax()
        group = group[group['SIDO'] == majority_sido]
    return group

# Group top_recommendations_with_sido by 'userID' and apply the filter_majority_sido function
filtered_top_recommendations = top_recommendations_with_sido.groupby('userID').apply(filter_majority_sido)

# Reset index after applying the function
filtered_top_recommendations.reset_index(drop=True, inplace=True)

filtered_top_recommendations.to_csv(os.path.join(SAVEMODEL_DIR, 'result/final_output/G_top_recommendations.csv'))

from datetime import datetime

# Get the current timestamp
timestamp = datetime.now()

# Print the timestamp
print("Current Timestamp:", timestamp)